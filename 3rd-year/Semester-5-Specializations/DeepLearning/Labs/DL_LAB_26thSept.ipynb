{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fkHIECxZGSk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Matrix Multiplication\n",
        "\n",
        "## Using NumPy\n",
        "\n",
        "A_np = np.array([[78, 11, 56], [92, 67, 45], [8, 99, 71]])\n",
        "B_np = np.array([[23, 78, 11], [14, 67, 45], [8, 34, 71]])\n",
        "\n",
        "start = time.time()\n",
        "C_np = A_np @ B_np\n",
        "end = time.time()\n",
        "\n",
        "print(f\"NumPy time: {end - start:.4f} sec\")\n",
        "\n",
        "## Using PyTorch (CPU)\n",
        "\n",
        "A_torch = torch.tensor([[78, 11, 56], [92, 67, 45], [8, 99, 71]], dtype=torch.float)\n",
        "B_torch = torch.tensor([[23, 78, 11], [14, 67, 45], [8, 34, 71]], dtype=torch.float)\n",
        "\n",
        "start = time.time()\n",
        "C_torch = A_torch @ B_torch\n",
        "end = time.time()\n",
        "\n",
        "print(f\"PyTorch time: {end - start:.4f} sec\")\n",
        "\n",
        "\n",
        "# Using PyTorch (GPU, if available)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "A_torch_gpu = A_torch.to(device)\n",
        "B_torch_gpu = B_torch.to(device)\n",
        "if device == \"cuda\":\n",
        "    start = time.time()\n",
        "    C_torch_gpu = torch.matmul(A_torch_gpu, B_torch_gpu)\n",
        "    end = time.time()\n",
        "    print(f\"PyTorch (GPU) time: {end - start:.4f} sec\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jO2g2VdjaFkn",
        "outputId": "8926d288-44b7-4b9e-c43b-183a26ec6e60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy time: 0.0002 sec\n",
            "PyTorch time: 0.0002 sec\n",
            "PyTorch (GPU) time: 0.0002 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Time with CPU:\n",
        "NumPy time: 0.0002 sec \\\n",
        "PyTorch time: 0.0454 sec\n",
        "\n",
        "### Time with GPU:\n",
        "NumPy time: 0.0001 sec \\\n",
        "PyTorch time: 0.0002 sec \\\n",
        "PyTorch (GPU) time: 0.0001 sec"
      ],
      "metadata": {
        "id": "7yNA0trxobEF"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab1f8919",
        "outputId": "700ea55d-c5c9-4a32-848b-34126b95943f"
      },
      "source": [
        "# Array Broadcasting\n",
        "\n",
        "# Scalar and Array\n",
        "a = np.array([1, 2, 3])\n",
        "print(a.shape)\n",
        "b = 2\n",
        "print(\"Scalar and Array:\")\n",
        "c = a + b\n",
        "print(c)\n",
        "print(\"Output shape \\n\", c.shape)\n",
        "print(\"---------------------------------\")\n",
        "\n",
        "# (m,1) + (1,n) -> (m,n)\n",
        "a = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "print(a.shape)\n",
        "b = np.array([10, 20, 30])\n",
        "print(b.shape)\n",
        "print(\"Compatible Shapes:\")\n",
        "c = a + b\n",
        "print(c)\n",
        "print(\"Output shape \\n\", c.shape)\n",
        "print(\"---------------------------------\")\n",
        "\n",
        "# (m,n) + (n,) -> (m,n)\n",
        "a = np.array([[1, 2], [3, 4], [5, 6]])\n",
        "print(a.shape)\n",
        "b = np.array([[10, 20]])\n",
        "print(b.shape)\n",
        "print(\"Another Compatible Shape:\")\n",
        "c = a + b\n",
        "print(c)\n",
        "print(\"Output shape \\n\", c.shape)\n",
        "print(\"---------------------------------\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3,)\n",
            "Scalar and Array:\n",
            "[3 4 5]\n",
            "Output shape \n",
            " (3,)\n",
            "---------------------------------\n",
            "(2, 3)\n",
            "(3,)\n",
            "Compatible Shapes:\n",
            "[[11 22 33]\n",
            " [14 25 36]]\n",
            "Output shape \n",
            " (2, 3)\n",
            "---------------------------------\n",
            "(3, 2)\n",
            "(1, 2)\n",
            "Another Compatible Shape:\n",
            "[[11 22]\n",
            " [13 24]\n",
            " [15 26]]\n",
            "Output shape \n",
            " (3, 2)\n",
            "---------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bbb8ec7",
        "outputId": "2a0b3e53-bfc6-4926-9c21-4f803f25105e"
      },
      "source": [
        "## Here Broadcasting Fails\n",
        "\n",
        "# Incompatible shapes\n",
        "a = np.array([[1, 2], [3, 4]])\n",
        "print(a.shape)\n",
        "b = np.array([1, 2])\n",
        "print(b.shape)\n",
        "print(\"Incompatible Shapes - Fails:\")\n",
        "try:\n",
        "    print(a + b)\n",
        "except ValueError as e:\n",
        "    print(f\"Error: {e}\")\n",
        "print(\"---------------------------------\")\n",
        "\n",
        "# Another incompatible shape example\n",
        "a = np.array([1, 2, 3])\n",
        "print(a.shape)\n",
        "b = np.array([[1], [2]])\n",
        "print(b.shape)\n",
        "print(\"Another Incompatible Shape - Fails:\")\n",
        "try:\n",
        "    print(a + b)\n",
        "except ValueError as e:\n",
        "    print(f\"Error: {e}\")\n",
        "print(\"---------------------------------\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 2)\n",
            "(2,)\n",
            "Incompatible Shapes - Fails:\n",
            "[[2 4]\n",
            " [4 6]]\n",
            "---------------------------------\n",
            "(3,)\n",
            "(2, 1)\n",
            "Another Incompatible Shape - Fails:\n",
            "[[2 3 4]\n",
            " [3 4 5]]\n",
            "---------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Conclusion is\n",
        "# -> Works when dimensions are equal or one is 1 (Scalar).\n",
        "# ->Fails otherwise."
      ],
      "metadata": {
        "id": "fs--B7vvl5Jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Activation Function\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    s = sigmoid(x)\n",
        "    return s * (1 - s)\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh_derivative(x):\n",
        "    return 1 - np.tanh(x)**2\n",
        "\n",
        "def leaky_relu(x, alpha=0.01):\n",
        "    return np.maximum(alpha * x, x)\n",
        "\n",
        "def leaky_relu_derivative(x, alpha=0.01):\n",
        "    dx = np.ones_like(x)\n",
        "    dx[x < 0] = alpha\n",
        "    return dx\n",
        "\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)"
      ],
      "metadata": {
        "id": "ZEUEWYS0cQCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6f89abf",
        "outputId": "55f5e4d6-57b2-4856-e4bf-fb5d5c87208c"
      },
      "source": [
        "# Example Usage of Activation Functions\n",
        "\n",
        "x = np.array([-2, -1, 0, 1, 2], dtype=float)\n",
        "print(x)\n",
        "\n",
        "print(\"ReLU:\")\n",
        "print(\"Output:\", relu(x))\n",
        "print(\"Derivative:\", relu_derivative(x))\n",
        "print(\"---------------------------------\")\n",
        "\n",
        "print(\"Sigmoid:\")\n",
        "print(\"Output:\", sigmoid(x))\n",
        "print(\"Derivative:\", sigmoid_derivative(x))\n",
        "print(\"---------------------------------\")\n",
        "\n",
        "print(\"Tanh:\")\n",
        "print(\"Output:\", tanh(x))\n",
        "print(\"Derivative:\", tanh_derivative(x))\n",
        "print(\"---------------------------------\")\n",
        "\n",
        "alpha_leaky = 0.1\n",
        "print(f\"Leaky ReLU (alpha={alpha_leaky}):\")\n",
        "print(\"Output:\", leaky_relu(x, alpha=alpha_leaky))\n",
        "print(\"Derivative:\", leaky_relu_derivative(x, alpha=alpha_leaky))\n",
        "print(\"---------------------------------\")\n",
        "\n",
        "x_new = np.array([1.0, 2.0, 3.0])\n",
        "print(\"Softmax:\")\n",
        "print(\"Input:\", x_new)\n",
        "print(\"Output:\", softmax(x_new))\n",
        "print(\"---------------------------------\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-2. -1.  0.  1.  2.]\n",
            "ReLU:\n",
            "Output: [0. 0. 0. 1. 2.]\n",
            "Derivative: [0. 0. 0. 1. 1.]\n",
            "---------------------------------\n",
            "Sigmoid:\n",
            "Output: [0.11920292 0.26894142 0.5        0.73105858 0.88079708]\n",
            "Derivative: [0.10499359 0.19661193 0.25       0.19661193 0.10499359]\n",
            "---------------------------------\n",
            "Tanh:\n",
            "Output: [-0.96402758 -0.76159416  0.          0.76159416  0.96402758]\n",
            "Derivative: [0.07065082 0.41997434 1.         0.41997434 0.07065082]\n",
            "---------------------------------\n",
            "Leaky ReLU (alpha=0.1):\n",
            "Output: [-0.2 -0.1  0.   1.   2. ]\n",
            "Derivative: [0.1 0.1 1.  1.  1. ]\n",
            "---------------------------------\n",
            "Softmax:\n",
            "Input: [1. 2. 3.]\n",
            "Output: [0.09003057 0.24472847 0.66524096]\n",
            "---------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xgEq-L6-pBEB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}