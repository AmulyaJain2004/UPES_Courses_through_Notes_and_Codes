{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PA9uUqNKJmA0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP:\n",
        "  def __init__(self, input_size, hidden_size, output_size, lr=0.1):\n",
        "    self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
        "    self.b1 = np.zeros((1, hidden_size))\n",
        "    self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
        "    self.b2 = np.zeros((1, output_size))\n",
        "    self.lr = lr\n",
        "\n",
        "  def mlp_stats(self):\n",
        "    print(f\"W1 shape: {self.W1.shape}\")\n",
        "    print(f\"b1 shape: {self.b1.shape}\")\n",
        "    print(f\"W2 shape: {self.W2.shape}\")\n",
        "    print(f\"b2 shape: {self.b2.shape}\")\n",
        "\n",
        "  def sigmoid(self, x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "  def sigmoid_deriv(self, x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "  def forward(self, X):\n",
        "    self.z1 = np.dot(X, self.W1) + self.b1\n",
        "    self.a1 = self.sigmoid(self.z1)\n",
        "\n",
        "    self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
        "    self.a2 = self.sigmoid(self.z2)\n",
        "\n",
        "    return self.a2\n",
        "\n",
        "  def backward(self, X, y):\n",
        "    m = y.shape[0]\n",
        "\n",
        "    dz2 = (self.a2 - y) * self.sigmoid_deriv(self.a2)\n",
        "    dW2 = np.dot(self.a1.T, dz2) / m\n",
        "    db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
        "\n",
        "    dz1 = np.dot(dz2, self.W2.T) * self.sigmoid_deriv(self.a1)\n",
        "    dW1 = np.dot(X.T, dz1) / m\n",
        "    db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
        "\n",
        "    self.W1 -= self.lr * dW1\n",
        "    self.b1 -= self.lr * db1\n",
        "    self.W2 -= self.lr * dW2\n",
        "    self.b2 -= self.lr * db2\n",
        "\n",
        "  def train(self, X, y, epochs):\n",
        "    for epoch in range(epochs):\n",
        "      output = self.forward(X)\n",
        "      loss = np.mean((y - output) ** 2)\n",
        "      self.backward(X, y)\n",
        "\n",
        "      if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n"
      ],
      "metadata": {
        "id": "uG11Nl6NOplI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "mlp = MLP(input_size=2, hidden_size=4, output_size=1, lr=0.1)\n",
        "mlp.mlp_stats()\n",
        "mlp.train(X, y, epochs=2000)\n",
        "mlp.mlp_stats()\n",
        "\n",
        "print(\"Predictions:\")\n",
        "print(mlp.forward(X))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6p7fx3NVuPI",
        "outputId": "c431e754-cb9e-4492-9a02-4ef585acdbc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W1 shape: (2, 4)\n",
            "b1 shape: (1, 4)\n",
            "W2 shape: (4, 1)\n",
            "b2 shape: (1, 1)\n",
            "Epoch 0, Loss: 0.2500043812091003\n",
            "Epoch 100, Loss: 0.2500003568443404\n",
            "Epoch 200, Loss: 0.25000002894848095\n",
            "Epoch 300, Loss: 0.2500000022330118\n",
            "Epoch 400, Loss: 0.25000000005635503\n",
            "Epoch 500, Loss: 0.24999999987900648\n",
            "Epoch 600, Loss: 0.24999999986455212\n",
            "Epoch 700, Loss: 0.2499999998633697\n",
            "Epoch 800, Loss: 0.24999999986326862\n",
            "Epoch 900, Loss: 0.2499999998632556\n",
            "Epoch 1000, Loss: 0.24999999986324978\n",
            "Epoch 1100, Loss: 0.24999999986324453\n",
            "Epoch 1200, Loss: 0.2499999998632393\n",
            "Epoch 1300, Loss: 0.24999999986323415\n",
            "Epoch 1400, Loss: 0.2499999998632289\n",
            "Epoch 1500, Loss: 0.24999999986322374\n",
            "Epoch 1600, Loss: 0.24999999986321852\n",
            "Epoch 1700, Loss: 0.24999999986321336\n",
            "Epoch 1800, Loss: 0.2499999998632082\n",
            "Epoch 1900, Loss: 0.24999999986320298\n",
            "W1 shape: (2, 4)\n",
            "b1 shape: (1, 4)\n",
            "W2 shape: (4, 1)\n",
            "b2 shape: (1, 1)\n",
            "Predictions:\n",
            "[[0.49999724]\n",
            " [0.49999201]\n",
            " [0.50000798]\n",
            " [0.50000274]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jm8NNTPudlP8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}